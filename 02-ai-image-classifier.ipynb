{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abadefad",
   "metadata": {},
   "source": [
    "# Real or AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b892dd8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:26.566712Z",
     "iopub.status.busy": "2025-11-23T13:08:26.566450Z",
     "iopub.status.idle": "2025-11-23T13:08:41.917929Z",
     "shell.execute_reply": "2025-11-23T13:08:41.917132Z"
    },
    "papermill": {
     "duration": 15.357228,
     "end_time": "2025-11-23T13:08:41.919417",
     "exception": false,
     "start_time": "2025-11-23T13:08:26.562189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import timm\n",
    "# import albumentations as albu\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF # Explicit import for functional transforms\n",
    "import torch.fft # Added for Frequency Stream\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b39fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:41.926144Z",
     "iopub.status.busy": "2025-11-23T13:08:41.925475Z",
     "iopub.status.idle": "2025-11-23T13:08:41.931018Z",
     "shell.execute_reply": "2025-11-23T13:08:41.930424Z"
    },
    "papermill": {
     "duration": 0.009885,
     "end_time": "2025-11-23T13:08:41.932170",
     "exception": false,
     "start_time": "2025-11-23T13:08:41.922285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, transforms=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing file names and labels.\n",
    "            data_dir (str): Directory where the image files are stored.\n",
    "            transforms (callable, optional): Torchvision transforms to apply to the images.\n",
    "            is_train (bool, optional): Flag to indicate if the dataset is for training.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the file name and label\n",
    "        filename = self.df.iloc[idx]['file_name']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        # Load the image\n",
    "        img_path = os.path.join(self.data_dir, filename)\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "\n",
    "        # Apply transforms if specified\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # For training, return the image and label\n",
    "        if self.is_train:\n",
    "            return image, label\n",
    "        # For inference, return only the image\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddba1f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:41.938461Z",
     "iopub.status.busy": "2025-11-23T13:08:41.937858Z",
     "iopub.status.idle": "2025-11-23T13:08:41.941528Z",
     "shell.execute_reply": "2025-11-23T13:08:41.941008Z"
    },
    "papermill": {
     "duration": 0.007918,
     "end_time": "2025-11-23T13:08:41.942577",
     "exception": false,
     "start_time": "2025-11-23T13:08:41.934659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # Reduced to prevent OOM on P100\n",
    "SEED = 42\n",
    "IMG_SIZE = 384\n",
    "\n",
    "LOAD_PRETRAINED_WEIGHTS = False  # Set to True to load weights and resume training\n",
    "PRETRAINED_WEIGHTS_PATH = \"best_model_triple_v1.pth\"  # Path to checkpoint; accepts both state_dict and dict ckp formats\n",
    "RESUME_FROM_EPOCH = 0   # Optional override if checkpoint doesn't contain epoch info\n",
    "\n",
    "\n",
    "SAVE_DIR = \"checkpoints_triple_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd47079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:41.948705Z",
     "iopub.status.busy": "2025-11-23T13:08:41.948176Z",
     "iopub.status.idle": "2025-11-23T13:08:41.951420Z",
     "shell.execute_reply": "2025-11-23T13:08:41.950860Z"
    },
    "papermill": {
     "duration": 0.007382,
     "end_time": "2025-11-23T13:08:41.952420",
     "exception": false,
     "start_time": "2025-11-23T13:08:41.945038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "NUM_EPOCHS = 15 # Increased for Curriculum Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1103b019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:41.958370Z",
     "iopub.status.busy": "2025-11-23T13:08:41.957891Z",
     "iopub.status.idle": "2025-11-23T13:08:44.711090Z",
     "shell.execute_reply": "2025-11-23T13:08:44.710116Z"
    },
    "papermill": {
     "duration": 2.757925,
     "end_time": "2025-11-23T13:08:44.712703",
     "exception": false,
     "start_time": "2025-11-23T13:08:41.954778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the train CSV\n",
    "train_csv_path = \"/kaggle/input/ai-vs-human-generated-dataset/train.csv\"\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "train_data_dir = \"/kaggle/input/ai-vs-human-generated-dataset/\"\n",
    "\n",
    "# Phase 1 & 4: Data Preparation & Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop((IMG_SIZE, IMG_SIZE), pad_if_needed=True), # Phase 1: Random Crops (No Resize)\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5), # Phase 4: Flips\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)), # Phase 4: Cutout/Dropout\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((IMG_SIZE, IMG_SIZE)), # Phase 1: Center Crop for Val\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "# Split: 0.7 Train, 0.2 Val, 0.1 Test\n",
    "# First split: 0.7 Train, 0.3 Temp\n",
    "train_df, temp_df = train_test_split(train_df, test_size=0.3, random_state=SEED, stratify=train_df['label'])\n",
    "# Second split: Split Temp (0.3) into Val (0.2) and Test (0.1). \n",
    "# 0.1 is 1/3 of 0.3, so test_size=1/3\n",
    "val_df, test_df = train_test_split(temp_df, test_size=1/3, random_state=SEED, stratify=temp_df['label'])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = CustomDataset(train_df, train_data_dir, transforms=train_transform, is_train=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataset = CustomDataset(val_df, train_data_dir, transforms=val_transform, is_train=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset = CustomDataset(test_df, train_data_dir, transforms=val_transform, is_train=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for images, labels in train_dataloader:\n",
    "    print(images.shape, labels)\n",
    "    break\n",
    "\n",
    "for images, labels in val_dataloader:\n",
    "    print(images.shape, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33683406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:44.719638Z",
     "iopub.status.busy": "2025-11-23T13:08:44.719028Z",
     "iopub.status.idle": "2025-11-23T13:08:46.334499Z",
     "shell.execute_reply": "2025-11-23T13:08:46.333786Z"
    },
    "papermill": {
     "duration": 1.64152,
     "end_time": "2025-11-23T13:08:46.357029",
     "exception": false,
     "start_time": "2025-11-23T13:08:44.715509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Check Dataset Balance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "label_counts = train_df['label'].value_counts()\n",
    "print(\"Dataset Balance:\")\n",
    "print(label_counts)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Add Data visualization Strategies - Show samples images with IDX\n",
    "def show_samples(df, data_dir, num_samples=8, cols=4):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    samples = df.sample(num_samples)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(samples.iterrows()):\n",
    "        img_path = os.path.join(data_dir, row['file_name'])\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            label = row['label']\n",
    "            \n",
    "            plt.subplot(num_samples // cols + (1 if num_samples % cols else 0), cols, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"ID: {row['file_name']}\\nLabel: {label}\")\n",
    "            plt.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {img_path}: {e}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample Images:\")\n",
    "show_samples(train_df, train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6659c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:46.399809Z",
     "iopub.status.busy": "2025-11-23T13:08:46.398777Z",
     "iopub.status.idle": "2025-11-23T13:08:50.585145Z",
     "shell.execute_reply": "2025-11-23T13:08:50.584435Z"
    },
    "papermill": {
     "duration": 4.209915,
     "end_time": "2025-11-23T13:08:50.586565",
     "exception": false,
     "start_time": "2025-11-23T13:08:46.376650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Phase 2: Architecture Strategy (The 3-Headed Monster)\n",
    "class ThreeStreamModel(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        # Stream A (Content): ConvNeXt\n",
    "        self.stream_a = timm.create_model('resnet18d.ra2_in1k', pretrained=True, num_classes=0)\n",
    "        \n",
    "        # Stream B (Noise): ResNet18\n",
    "        self.stream_b = timm.create_model('resnet18d.ra2_in1k', pretrained=True, num_classes=0)\n",
    "        \n",
    "        # Stream C (Frequency): ResNet18 (1 channel input)\n",
    "        self.stream_c = timm.create_model('resnet18', pretrained=True, num_classes=0, in_chans=1)\n",
    "        \n",
    "        # Fusion\n",
    "        in_features = self.stream_a.num_features + self.stream_b.num_features + self.stream_c.num_features\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def get_residual(self, x):\n",
    "        # Calculate Residual (Image - GaussianBlur)\n",
    "        blurred = TF.gaussian_blur(x, kernel_size=5, sigma=2.0)\n",
    "        return x - blurred\n",
    "\n",
    "    def get_spectrum(self, x):\n",
    "        # Calculate FFT Log-Spectrum\n",
    "        gray = TF.rgb_to_grayscale(x)\n",
    "        fft = torch.fft.fft2(gray)\n",
    "        fft_shift = torch.fft.fftshift(fft)\n",
    "        magnitude = 20 * torch.log(torch.abs(fft_shift) + 1e-8)\n",
    "        return magnitude\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Stream A: Raw RGB\n",
    "        feat_a = self.stream_a(x)\n",
    "        \n",
    "        # Stream B: Noise/Residual (Calculated internally)\n",
    "        res = self.get_residual(x)\n",
    "        feat_b = self.stream_b(res)\n",
    "        \n",
    "        # Stream C: Frequency (Calculated internally)\n",
    "        freq = self.get_spectrum(x)\n",
    "        feat_c = self.stream_c(freq)\n",
    "        \n",
    "        # Fusion\n",
    "        combined = torch.cat([feat_a, feat_b, feat_c], dim=1)\n",
    "        x = self.head(combined)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed34b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ThreeStreamModel\n",
    "model = ThreeStreamModel(num_classes=2)\n",
    "\n",
    "# Define Learning Rates\n",
    "BACKBONE_LR = LR  # 1e-5\n",
    "HEAD_LR = 1e-3    # Higher LR for the new head\n",
    "\n",
    "optimizer = Adam([\n",
    "    {'params': model.stream_a.parameters(), 'lr': BACKBONE_LR},\n",
    "    {'params': model.stream_b.parameters(), 'lr': BACKBONE_LR},\n",
    "    {'params': model.stream_c.parameters(), 'lr': BACKBONE_LR},\n",
    "    {'params': model.head.parameters(), 'lr': HEAD_LR}\n",
    "], weight_decay=1e-2)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# 2. Implement a Strategy - Class Weights\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Dataset is balanced, so we don't need class weights\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde2d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:50.631657Z",
     "iopub.status.busy": "2025-11-23T13:08:50.631051Z",
     "iopub.status.idle": "2025-11-23T13:08:50.652486Z",
     "shell.execute_reply": "2025-11-23T13:08:50.651879Z"
    },
    "papermill": {
     "duration": 0.044987,
     "end_time": "2025-11-23T13:08:50.653554",
     "exception": false,
     "start_time": "2025-11-23T13:08:50.608567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, loader, device, num_images=16):\n",
    "    \"\"\"\n",
    "    Visualizes a grid of predictions from the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                if images_shown >= num_images:\n",
    "                    break\n",
    "                \n",
    "                img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                # Denormalize\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                \n",
    "                plt.subplot(4, 4, images_shown + 1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"True: {labels[i].item()}\\nPred: {preds[i].item()}\", \n",
    "                          color=(\"green\" if labels[i]==preds[i] else \"red\"))\n",
    "                plt.axis('off')\n",
    "                images_shown += 1\n",
    "            \n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def run_validation_step(model, val_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "            # Store labels and predictions for F1 score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "def run_training_loop(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, loss_fn, device,\n",
    "    num_epochs, save_path=\"best_model.pth\", save_dir=\"checkpoints\",\n",
    "    resume=False, checkpoint_path=None, start_epoch=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates it on the validation set. Supports resuming from checkpoints.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Try to load checkpoint if requested\n",
    "    if resume and checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
    "        ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "        if isinstance(ckpt, dict):\n",
    "            try:\n",
    "                if 'model_state_dict' in ckpt:\n",
    "                    model.load_state_dict(ckpt['model_state_dict'])\n",
    "                else:\n",
    "                    model.load_state_dict(ckpt)\n",
    "\n",
    "                # Load optimizer/scheduler if present\n",
    "                if 'optimizer_state_dict' in ckpt:\n",
    "                    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "                if 'scheduler_state_dict' in ckpt:\n",
    "                    try:\n",
    "                        scheduler.load_state_dict(ckpt['scheduler_state_dict'])\n",
    "                    except Exception as e:\n",
    "                        print('Could not load scheduler state:', e)\n",
    "\n",
    "                # Determine start epoch\n",
    "                start_epoch = ckpt.get('epoch', start_epoch) + 1 if ckpt.get('epoch') is not None else start_epoch\n",
    "                print(f\"Resuming from epoch: {start_epoch}\")\n",
    "            except Exception as e:\n",
    "                print('Could not fully restore checkpoint; starting from start_epoch. Error:', e)\n",
    "        else:\n",
    "            model.load_state_dict(ckpt)\n",
    "    else:\n",
    "        # no resume or checkpoint not found, start at provided start_epoch\n",
    "        print('Starting training from scratch' if not resume else f'Checkpoint {checkpoint_path} not found; starting from {start_epoch}')\n",
    "\n",
    "    # Keep track of the best model\n",
    "    best_f1 = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n",
    "\n",
    "    curr_step = 0\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | starting epoch {epoch}\")\n",
    "        \n",
    "        # Phase 3: Curriculum Training\n",
    "        if epoch < 4:\n",
    "            print(\"Phase: Warmup (Head Only)\")\n",
    "            for param in model.parameters(): param.requires_grad = False\n",
    "            for param in model.head.parameters(): param.requires_grad = True\n",
    "        elif epoch < 11:\n",
    "            print(\"Phase: Noise Focus (Streams B, C & Head)\")\n",
    "            for param in model.parameters(): param.requires_grad = False\n",
    "            for param in model.head.parameters(): param.requires_grad = True\n",
    "            for param in model.stream_b.parameters(): param.requires_grad = True\n",
    "            for param in model.stream_c.parameters(): param.requires_grad = True\n",
    "        else:\n",
    "            print(\"Phase: Full Tuning\")\n",
    "            for param in model.parameters(): param.requires_grad = True\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        # Training phase\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (images, labels) in tqdm(enumerate(train_loader), desc=\"Training\", total=len(train_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute training metrics (accumulate until checkpoint event)\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            curr_step += 1\n",
    "\n",
    "            # Validate periodically\n",
    "            if curr_step % 1000 == 0:\n",
    "                val_loss, val_acc, val_f1 = run_validation_step(model, val_loader, loss_fn, device)\n",
    "\n",
    "                # Save the best model (full checkpoint including optimizer/scheduler states)\n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': getattr(scheduler, 'state_dict', lambda: {})(),\n",
    "                    }, save_path)\n",
    "                    print(f\"New best model saved with F1 score: {best_f1:.4f}\")\n",
    "\n",
    "                train_loss /= 1000 if curr_step >= 1000 else max(1, len(train_loader))\n",
    "                history[\"train_loss\"].append(train_loss)\n",
    "                history[\"val_loss\"].append(val_loss)\n",
    "                history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "                train_acc = train_correct / train_total if train_total > 0 else 0.0\n",
    "                print(f\"Step {curr_step}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "                print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "                print(\"Visualizing predictions...\")\n",
    "                visualize_predictions(model, val_loader, device)\n",
    "\n",
    "                train_loss = 0.0\n",
    "                train_total = 0\n",
    "                train_correct = 0\n",
    "                model.train()\n",
    "\n",
    "        # End of epoch: validate and save epoch checkpoint\n",
    "        print(f\"End of Epoch {epoch + 1} Validation\")\n",
    "        val_loss, val_acc, val_f1 = run_validation_step(model, val_loader, loss_fn, device)\n",
    "\n",
    "        epoch_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': getattr(scheduler, 'state_dict', lambda: {})(),\n",
    "        }, epoch_save_path)\n",
    "        print(f\"Saved checkpoint: {epoch_save_path}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': getattr(scheduler, 'state_dict', lambda: {})(),\n",
    "            }, save_path)\n",
    "            print(f\"New best model saved with F1 score: {best_f1:.4f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "    torch.save(best_model_wts, save_path)\n",
    "    print(f\"Best model weights saved to {save_path}\")\n",
    "    return best_model_wts, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bcb090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T13:08:50.699442Z",
     "iopub.status.busy": "2025-11-23T13:08:50.698735Z",
     "iopub.status.idle": "2025-11-23T15:18:13.050870Z",
     "shell.execute_reply": "2025-11-23T15:18:13.049933Z"
    },
    "papermill": {
     "duration": 7762.376193,
     "end_time": "2025-11-23T15:18:13.052350",
     "exception": false,
     "start_time": "2025-11-23T13:08:50.676157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_wts, history = run_training_loop(\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    save_path=PRETRAINED_WEIGHTS_PATH,\n",
    "    save_dir=SAVE_DIR,\n",
    "    resume=False,\n",
    "    checkpoint_path=PRETRAINED_WEIGHTS_PATH,\n",
    "    start_epoch=RESUME_FROM_EPOCH\n",
    ")\n",
    "\n",
    "# Load the best model weights after training (if needed)pip install pytorch-gradcam\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfaee6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:18:13.658721Z",
     "iopub.status.busy": "2025-11-23T15:18:13.657801Z",
     "iopub.status.idle": "2025-11-23T15:18:14.056995Z",
     "shell.execute_reply": "2025-11-23T15:18:14.056232Z"
    },
    "papermill": {
     "duration": 0.651634,
     "end_time": "2025-11-23T15:18:14.059552",
     "exception": false,
     "start_time": "2025-11-23T15:18:13.407918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_history(history):\n",
    "    \"\"\"\n",
    "    Visualizes the training and validation history.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Dictionary containing training and validation metrics with keys:\n",
    "                        - \"train_loss\": List of training loss values.\n",
    "                        - \"val_loss\": List of validation loss values.\n",
    "                        - \"val_f1\": List of validation F1 scores.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\", marker=\"o\")\n",
    "    plt.title(\"Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot validation F1 score\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history[\"val_f1\"], label=\"Val F1 Score\", marker=\"o\", color=\"green\")\n",
    "    plt.title(\"Validation F1 Score Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_history(history)\n",
    "\n",
    "# --- Evaluation on Train, Val, and Test Sets ---\n",
    "\n",
    "def evaluate_performance(model, loader, device, dataset_name=\"Dataset\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=f\"Evaluating {dataset_name}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"--- {dataset_name} Performance ---\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds))\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - {dataset_name}\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Evaluating on Train Set...\")\n",
    "evaluate_performance(model, train_dataloader, device, \"Train\")\n",
    "print(\"==\"*50)\n",
    "\n",
    "print(\"Evaluating on Validation Set...\")\n",
    "evaluate_performance(model, val_dataloader, device, \"Validation\")\n",
    "print(\"==\"*50)\n",
    "\n",
    "print(\"\\nEvaluating on Test Set...\")\n",
    "evaluate_performance(model, test_dataloader, device, \"Test\")\n",
    "print(\"==\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6761fae",
   "metadata": {
    "papermill": {
     "duration": 0.240412,
     "end_time": "2025-11-23T15:18:14.553813",
     "exception": false,
     "start_time": "2025-11-23T15:18:14.313401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c467b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:18:15.044280Z",
     "iopub.status.busy": "2025-11-23T15:18:15.043430Z",
     "iopub.status.idle": "2025-11-23T15:18:15.083474Z",
     "shell.execute_reply": "2025-11-23T15:18:15.082892Z"
    },
    "papermill": {
     "duration": 0.284374,
     "end_time": "2025-11-23T15:18:15.084658",
     "exception": false,
     "start_time": "2025-11-23T15:18:14.800284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the test CSV for predictions\n",
    "test_csv_path = \"/kaggle/input/ai-vs-human-generated-dataset/test.csv\"\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Define the base directory where images are stored\n",
    "test_data_dir = \"/kaggle/input/ai-vs-human-generated-dataset\"\n",
    "\n",
    "# Update image paths in the test dataframe\n",
    "test_df['img_path'] = test_df['id'].apply(lambda x: os.path.join(test_data_dir, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58df2af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:18:15.569703Z",
     "iopub.status.busy": "2025-11-23T15:18:15.568926Z",
     "iopub.status.idle": "2025-11-23T15:23:19.669131Z",
     "shell.execute_reply": "2025-11-23T15:23:19.668135Z"
    },
    "papermill": {
     "duration": 304.342082,
     "end_time": "2025-11-23T15:23:19.670346",
     "exception": false,
     "start_time": "2025-11-23T15:18:15.328264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize a list to store results\n",
    "predictions = []\n",
    "\n",
    "# Ensure model is on the correct device and in eval mode\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Phase 5: Inference (5-Crop Method)\n",
    "five_crop_transform = transforms.Compose([\n",
    "    transforms.FiveCrop(IMG_SIZE),\n",
    "    transforms.Lambda(lambda crops: torch.stack([\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))(transforms.ToTensor()(crop)) \n",
    "        for crop in crops\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Perform inference on test images\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    # if idx==100:\n",
    "    #     break\n",
    "    \n",
    "    img_path = row['img_path']  # Path to the image\n",
    "    id_ = row['id']\n",
    "    try:\n",
    "        # Open and preprocess image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply 5-Crop transform\n",
    "        # Shape: (5, C, H, W)\n",
    "        input_tensor = five_crop_transform(image).to(device)\n",
    "\n",
    "        # Model prediction\n",
    "        with torch.no_grad():\n",
    "            # Process all 5 crops\n",
    "            outputs = model(input_tensor) # (5, num_classes)\n",
    "            # Average probabilities\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            avg_probs = torch.mean(probs, dim=0)\n",
    "            predicted_label = torch.argmax(avg_probs).item()\n",
    "\n",
    "        predictions.append((id_, predicted_label))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bbbb4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T15:23:20.295099Z",
     "iopub.status.busy": "2025-11-23T15:23:20.294233Z",
     "iopub.status.idle": "2025-11-23T15:23:20.320526Z",
     "shell.execute_reply": "2025-11-23T15:23:20.319749Z"
    },
    "papermill": {
     "duration": 0.332826,
     "end_time": "2025-11-23T15:23:20.321668",
     "exception": false,
     "start_time": "2025-11-23T15:23:19.988842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for submission\n",
    "submission_df = pd.DataFrame(predictions, columns=[\"id\", \"label\"])\n",
    "\n",
    "# Save to CSV for submission\n",
    "submission_csv_path = \"submission.csv\"\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "print(f\"Submission file saved at {submission_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6412205,
     "sourceId": 10550636,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8100.044916,
   "end_time": "2025-11-23T15:23:22.853109",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-23T13:08:22.808193",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0054877b03134ba5ac114cb6e714fbae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9c8c3060feb14f58bbc9169cabb54162",
       "placeholder": "​",
       "style": "IPY_MODEL_85883847f29745c9957d41efe1cea627",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "008befdfd2434e20902b4828867b85c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "088c49372fef43ee9c73d101273a7d45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_61470709d5d94ec5bf8b68672a55594e",
       "max": 354400320,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_377e54070d8245e5a76629e37802f462",
       "tabbable": null,
       "tooltip": null,
       "value": 354400320
      }
     },
     "377e54070d8245e5a76629e37802f462": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3aa56865d527419aa8d57a8ff68567ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0054877b03134ba5ac114cb6e714fbae",
        "IPY_MODEL_088c49372fef43ee9c73d101273a7d45",
        "IPY_MODEL_60c4376e69674a65a8d4bbfabff0b753"
       ],
       "layout": "IPY_MODEL_008befdfd2434e20902b4828867b85c5",
       "tabbable": null,
       "tooltip": null
      }
     },
     "60c4376e69674a65a8d4bbfabff0b753": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f4bdb398efe047348ddbf5d183f9fcd6",
       "placeholder": "​",
       "style": "IPY_MODEL_7a86ee76fc654fd382fee990827df2dd",
       "tabbable": null,
       "tooltip": null,
       "value": " 354M/354M [00:02&lt;00:00, 244MB/s]"
      }
     },
     "61470709d5d94ec5bf8b68672a55594e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a86ee76fc654fd382fee990827df2dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "85883847f29745c9957d41efe1cea627": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9c8c3060feb14f58bbc9169cabb54162": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4bdb398efe047348ddbf5d183f9fcd6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
