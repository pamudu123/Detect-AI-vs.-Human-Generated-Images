{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla classification baseline with timm and albumentations \n",
    "\n",
    "- **timm** is a powerful library with various models pretrained on diverse datasets\n",
    "- **albumentations** is a library for image transformations & augmentations\n",
    "\n",
    "**Curr progress**:\n",
    "\n",
    "- DINOv2 small: 99.97% [Top 3]\n",
    "- Hiera small: 99.94%\n",
    "- EfficienNet V2 small: [last version]\n",
    "\n",
    "please upvote if you find this notebook helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T04:02:23.034127Z",
     "iopub.status.busy": "2025-11-23T04:02:23.033638Z",
     "iopub.status.idle": "2025-11-23T04:02:23.041953Z",
     "shell.execute_reply": "2025-11-23T04:02:23.040671Z",
     "shell.execute_reply.started": "2025-11-23T04:02:23.034088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import timm\n",
    "# import albumentations as albu\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW, Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T04:02:30.952403Z",
     "iopub.status.busy": "2025-11-23T04:02:30.951999Z",
     "iopub.status.idle": "2025-11-23T04:02:30.960417Z",
     "shell.execute_reply": "2025-11-23T04:02:30.959092Z",
     "shell.execute_reply.started": "2025-11-23T04:02:30.952372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, data_dir, transforms=None, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing file names and labels.\n",
    "            data_dir (str): Directory where the image files are stored.\n",
    "            transforms (callable, optional): Torchvision transforms to apply to the images.\n",
    "            is_train (bool, optional): Flag to indicate if the dataset is for training.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the file name and label\n",
    "        filename = self.df.iloc[idx]['file_name']\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        # Load the image\n",
    "        img_path = os.path.join(self.data_dir, filename)\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "        # image = np.array(image)  # REMOVED: Torchvision transforms work with PIL images\n",
    "\n",
    "        # Apply transforms if specified\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        # For training, return the image and label\n",
    "        if self.is_train:\n",
    "            return image, label\n",
    "        # For inference, return only the image\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T09:35:19.019246Z",
     "iopub.status.busy": "2025-01-22T09:35:19.018984Z",
     "iopub.status.idle": "2025-01-22T09:35:21.819342Z",
     "shell.execute_reply": "2025-01-22T09:35:21.818478Z",
     "shell.execute_reply.started": "2025-01-22T09:35:19.019222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SEED = 42\n",
    "IMG_SIZE = 384\n",
    "\n",
    "# Load the train CSV\n",
    "train_csv_path = \"/kaggle/input/ai-vs-human-generated-dataset/train.csv\"\n",
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Define the base directory where images are stored\n",
    "train_data_dir = \"/kaggle/input/ai-vs-human-generated-dataset/\"\n",
    "\n",
    "# Replaced Albumentations with Torchvision Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=SEED, stratify=train_df['label'])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = CustomDataset(train_df, train_data_dir, transforms=train_transform, is_train=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataset = CustomDataset(val_df, train_data_dir, transforms=val_transform, is_train=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for images, labels in train_dataloader:\n",
    "    print(images.shape, labels)\n",
    "    break\n",
    "\n",
    "for images, labels in val_dataloader:\n",
    "    print(images.shape, labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check Dataset Balance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "label_counts = train_df['label'].value_counts()\n",
    "print(\"Dataset Balance:\")\n",
    "print(label_counts)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Add Data visualization Strategies - Show samples images with IDX\n",
    "def show_samples(df, data_dir, num_samples=8, cols=4):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    samples = df.sample(num_samples)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(samples.iterrows()):\n",
    "        img_path = os.path.join(data_dir, row['file_name'])\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            label = row['label']\n",
    "            \n",
    "            plt.subplot(num_samples // cols + (1 if num_samples % cols else 0), cols, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"ID: {row['file_name']}\\nLabel: {label}\")\n",
    "            plt.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {img_path}: {e}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample Images:\")\n",
    "show_samples(train_df, train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T09:35:21.820717Z",
     "iopub.status.busy": "2025-01-22T09:35:21.820397Z",
     "iopub.status.idle": "2025-01-22T09:35:23.81932Z",
     "shell.execute_reply": "2025-01-22T09:35:23.818619Z",
     "shell.execute_reply.started": "2025-01-22T09:35:21.820691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LR = 1e-5\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "# model = timm.create_model('vit_small_patch14_dinov2.lvd142m', \n",
    "#                           pretrained=True,\n",
    "#                          num_classes=2)\n",
    "\n",
    "# model = timm.create_model('tf_efficientnetv2_s.in21k_ft_in1k', \n",
    "#                           pretrained=True,\n",
    "#                          num_classes=2)\n",
    "\n",
    "# Using DaViT (Dual Attention Vision Transformer)\n",
    "model = timm.create_model('convnext_base.clip_laion2b_augreg_ft_in1k', \n",
    "                          pretrained=True,\n",
    "                         num_classes=2)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=LR, weight_decay=1e-2)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "# 2. Implement a Strategy - Class Weights\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset is balanced, so we don't need class weights\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T09:35:23.820434Z",
     "iopub.status.busy": "2025-01-22T09:35:23.820112Z",
     "iopub.status.idle": "2025-01-22T09:35:23.831572Z",
     "shell.execute_reply": "2025-01-22T09:35:23.830919Z",
     "shell.execute_reply.started": "2025-01-22T09:35:23.820405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(model, loader, device, num_images=16):\n",
    "    \"\"\"\n",
    "    Visualizes a grid of predictions from the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                if images_shown >= num_images:\n",
    "                    break\n",
    "                \n",
    "                img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "                # Denormalize\n",
    "                mean = np.array([0.485, 0.456, 0.406])\n",
    "                std = np.array([0.229, 0.224, 0.225])\n",
    "                img = std * img + mean\n",
    "                img = np.clip(img, 0, 1)\n",
    "                \n",
    "                plt.subplot(4, 4, images_shown + 1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"True: {labels[i].item()}\\nPred: {preds[i].item()}\", \n",
    "                          color=(\"green\" if labels[i]==preds[i] else \"red\"))\n",
    "                plt.axis('off')\n",
    "                images_shown += 1\n",
    "            \n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def validate_model(model, val_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "            # Store labels and predictions for F1 score calculation\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    return val_loss, val_acc, val_f1\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, optimizer, scheduler, loss_fn, device, num_epochs=10, save_path=\"best_model.pth\", save_dir=\"checkpoints\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates it on the validation set. Saves the model with the best F1 score.\n",
    "    \"\"\"\n",
    "    model.to(device)  # Move model to the specified device\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  # To store the best weights\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\n",
    "\n",
    "    curr_step = 0\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (images, labels) in tqdm(enumerate(train_loader), desc=\"Training\", total=len(train_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = loss_fn(outputs, labels)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Compute training metrics\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            curr_step += 1\n",
    "            \n",
    "            # Validate every 1000 steps\n",
    "            if curr_step % 1000 == 0:\n",
    "                val_loss, val_acc, val_f1 = validate_model(model, val_loader, loss_fn, device)\n",
    "        \n",
    "                # Save the best model weights based on validation F1 score\n",
    "                if val_f1 > best_f1:\n",
    "                    best_f1 = val_f1\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    print(f\"New best model saved with F1 score: {best_f1:.4f}\")\n",
    "\n",
    "                train_loss /= 1000\n",
    "                \n",
    "                # Log metrics\n",
    "                history[\"train_loss\"].append(train_loss)\n",
    "                history[\"val_loss\"].append(val_loss)\n",
    "                history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "                # Print results after 1k steps\n",
    "                train_acc = train_correct / train_total\n",
    "                print(f\"Step {curr_step}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "                print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "                \n",
    "                # 3. When Training show some samples grid predictions with actual labels (4x4 grid)\n",
    "                print(\"Visualizing predictions...\")\n",
    "                visualize_predictions(model, val_loader, device)\n",
    "\n",
    "                # reset\n",
    "                train_loss = 0.0\n",
    "                train_total = 0\n",
    "                train_correct = 0\n",
    "                \n",
    "                # Switch back to train mode\n",
    "                model.train()\n",
    "        \n",
    "        # End of Epoch Validation\n",
    "        print(f\"End of Epoch {epoch + 1} Validation\")\n",
    "        val_loss, val_acc, val_f1 = validate_model(model, val_loader, loss_fn, device)\n",
    "        \n",
    "        # Save model checkpoint for the current epoch\n",
    "        epoch_save_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
    "        torch.save(model.state_dict(), epoch_save_path)\n",
    "        print(f\"Saved checkpoint: {epoch_save_path}\")\n",
    "        \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"New best model saved with F1 score: {best_f1:.4f}\")\n",
    "            \n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    print(f\"Best F1 Score: {best_f1:.4f}\")\n",
    "    \n",
    "    # Ensure the best model is saved at the end\n",
    "    torch.save(best_model_wts, save_path)\n",
    "    print(f\"Best model weights saved to {save_path}\")\n",
    "    \n",
    "    return best_model_wts, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T09:36:13.959363Z",
     "iopub.status.busy": "2025-01-22T09:36:13.959019Z",
     "iopub.status.idle": "2025-01-22T09:39:15.649686Z",
     "shell.execute_reply": "2025-01-22T09:39:15.64847Z",
     "shell.execute_reply.started": "2025-01-22T09:36:13.959333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_model_wts, history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    save_path=\"best_model.pth\",\n",
    "    save_dir=\"checkpoints\"\n",
    ")\n",
    "\n",
    "# Load the best model weights after training\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T06:02:29.993945Z",
     "iopub.status.busy": "2025-01-21T06:02:29.993602Z",
     "iopub.status.idle": "2025-01-21T06:02:30.013381Z",
     "shell.execute_reply": "2025-01-21T06:02:30.012233Z",
     "shell.execute_reply.started": "2025-01-21T06:02:29.993911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_history(history):\n",
    "    \"\"\"\n",
    "    Visualizes the training and validation history.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Dictionary containing training and validation metrics with keys:\n",
    "                        - \"train_loss\": List of training loss values.\n",
    "                        - \"val_loss\": List of validation loss values.\n",
    "                        - \"val_f1\": List of validation F1 scores.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\", marker=\"o\")\n",
    "    plt.title(\"Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Plot validation F1 score\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history[\"val_f1\"], label=\"Val F1 Score\", marker=\"o\", color=\"green\")\n",
    "    plt.title(\"Validation F1 Score Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T06:02:32.628056Z",
     "iopub.status.busy": "2025-01-21T06:02:32.627752Z",
     "iopub.status.idle": "2025-01-21T06:02:32.686105Z",
     "shell.execute_reply": "2025-01-21T06:02:32.685494Z",
     "shell.execute_reply.started": "2025-01-21T06:02:32.628032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the test CSV for predictions\n",
    "test_csv_path = \"/kaggle/input/ai-vs-human-generated-dataset/test.csv\"\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Define the base directory where images are stored\n",
    "test_data_dir = \"/kaggle/input/ai-vs-human-generated-dataset\"\n",
    "\n",
    "# Update image paths in the test dataframe\n",
    "test_df['img_path'] = test_df['id'].apply(lambda x: os.path.join(test_data_dir, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T06:02:34.191989Z",
     "iopub.status.busy": "2025-01-21T06:02:34.191699Z",
     "iopub.status.idle": "2025-01-21T06:02:37.694802Z",
     "shell.execute_reply": "2025-01-21T06:02:37.694083Z",
     "shell.execute_reply.started": "2025-01-21T06:02:34.191967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize a list to store results\n",
    "predictions = []\n",
    "\n",
    "# Perform inference on test images\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    # if idx==100:\n",
    "    #     break\n",
    "    \n",
    "    img_path = row['img_path']  # Path to the image\n",
    "    id_ = row['id']\n",
    "    try:\n",
    "        # Open and preprocess image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        # Apply transforms directly to PIL image\n",
    "        input_tensor = val_transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "        # Model prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            predicted_label = torch.argmax(outputs, dim=-1).item()  # Get predicted class\n",
    "\n",
    "        predictions.append((id_, predicted_label))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T06:02:40.455914Z",
     "iopub.status.busy": "2025-01-21T06:02:40.455627Z",
     "iopub.status.idle": "2025-01-21T06:02:40.467034Z",
     "shell.execute_reply": "2025-01-21T06:02:40.466138Z",
     "shell.execute_reply.started": "2025-01-21T06:02:40.455891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for submission\n",
    "submission_df = pd.DataFrame(predictions, columns=[\"id\", \"label\"])\n",
    "\n",
    "# Save to CSV for submission\n",
    "submission_csv_path = \"submission.csv\"\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "print(f\"Submission file saved at {submission_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T06:02:40.691855Z",
     "iopub.status.busy": "2025-01-21T06:02:40.691585Z",
     "iopub.status.idle": "2025-01-21T06:02:40.701675Z",
     "shell.execute_reply": "2025-01-21T06:02:40.700961Z",
     "shell.execute_reply.started": "2025-01-21T06:02:40.69183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "value_counts = submission_df['label'].value_counts()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T06:02:41.657682Z",
     "iopub.status.busy": "2025-01-21T06:02:41.657237Z",
     "iopub.status.idle": "2025-01-21T06:02:41.671617Z",
     "shell.execute_reply": "2025-01-21T06:02:41.670783Z",
     "shell.execute_reply.started": "2025-01-21T06:02:41.657641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10884264,
     "sourceId": 91198,
     "sourceType": "competition"
    },
    {
     "datasetId": 6412205,
     "sourceId": 10550636,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
